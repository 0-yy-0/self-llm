# llm-QuicklyDeploy
基于AutoDL快速部署开源大模型，更适合中国宝宝的部署教程

# 模型

- InternLM
  - [x] InternLM-Chat-7B
  - [ ] Lagent+InternLM-Chat-7B-V1.1
  - [ ] 浦语灵笔图文理解&创作
- ChatGLM
  - [ ] ChatGLM2-6B
  - [ ] CogVlm
- Qwen
  - [ ] Qwen-7B
  - [ ] Qwen-VL
- 欢迎提交新模型
# 通用环境配置

## pip、conda 换源

## 开放端口

将 `autodl `的端口映射到本地的 [http://localhost:6006](http://localhost:6006/) 仅在此处展示一次，以下两个 Demo 都是同样的方法把 `autodl `中的 `6006 `端口映射到本机的 `http://localhost:6006`的方法都是相同的，方法如图所示。

![Alt text](images/image-4.png)

## 模型下载

### hugging face

### hugging face 镜像下载

### modelscope

### git-lfs

# 致谢

<div align=center>
  <a href="https://datawhale.club/#/">Datawhale</a>、
  <a href="https://www.shlab.org.cn/">上海人工智能实验室</a>
</div>